{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Performance Evaluation of Bandit Algorithms\n",
    "\n",
    "- In this project, you will implement several classical bandit algorithms, evluate their performance via numerical comparison and finally gain inspiring intuition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Classical Bandit Algorithms\n",
    "\n",
    "We consider a time-slotted bandit system ($t=1,2,\\ldots$) with three arms.\n",
    "We denote the arm set as $\\{1,2,3\\}$.\n",
    "Pulling each arm $j$ ($ j \\in \\{1,2,3\\}$) will obtain a random reward $r_{j}$, which follows a Bernoulli distribution with mean $\\theta_{j}$, *i.e.*, Bern($\\theta_{j}$).\n",
    "Specifically,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\tr_{j} = \n",
    "\t\t\\begin{cases}\n",
    "\t\t\t1, & w.p.\\ \\theta_{j}, \\\\\n",
    "\t\t\t0, & w.p.\\ 1-\\theta_{j},\t\t\t\n",
    "\t\t\\end{cases}\n",
    "\t\\end{aligned}\n",
    "\\end{equation*}\n",
    "where $\\theta_{j}, j \\in\\{1,2,3\\}$ are parameters within $(0,1)$.\n",
    "  \n",
    "Now we run this bandit system for $N$ ($N \\gg 3$) time slots.\n",
    "In each time slot $t$, we choose one and only one arm from these three arms, which we denote as $I(t) \\in \\{1,2,3\\}$.\n",
    "Then we pull the arm $I(t)$ and obtain a random reward $r_{I(t)}$.\n",
    "Our objective is to find an optimal policy to choose an arm $I(t)$ in each time slot $t$ such that the expectation of the aggregated reward over $N$ time slots is maximized, *i.e.*,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t = 1,\\dots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^{N} r_{I(t)} \\right].\n",
    "\t\\end{aligned}  \t\n",
    "\\end{equation*}\n",
    "\n",
    "If we know the values of $\\theta_{j},j \\in \\{1,2,3\\}$, this problem is trivial.\n",
    "Since $r_{I(t)} \\sim \\text{Bern}(\\theta_{I(t)})$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] \n",
    "\t\t= \\sum_{t=1}^{N} \\mathbb{E}[r_{I(t)}] \n",
    "\t\t= \\sum_{t=1}^N \\theta_{I(t)}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "Let $I(t) = I^{*} = \\mathop{\\arg \\max}\\limits_{ j \\in \\{1,2,3\\}} \\ \\theta_j$ for $t=1,2,\\ldots,N$, then \n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\begin{aligned}\n",
    "\t\t\\max_{I(t),t=1,\\ldots,N} \\ \\  \\mathbb{E}\\left[\\sum_{t=1}^N r_{I(t)} \\right] = N \\cdot \\theta_{I^*}.\n",
    "\t\\end{aligned} \t\n",
    "\\end{equation*}\n",
    "\n",
    "However, in reality, we do not know the values of $\\theta_{j},j \\in \\{1,2,3\\}$.\n",
    "We need to estimate the values $\\theta_{j}, j \\in \\{1,2,3\\}$ via empirical samples, and then make the decisions in each time slot. \n",
    "Next we introduce three classical bandit algorithms: $\\epsilon$-greedy, UCB, and TS, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-greedy Algorithm ($0 \\leq \\epsilon \\leq 1$)\n",
    "<img src=\"figures/e-greedy.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCB (Upper Confidence Bound) Algorithm\n",
    "<img src=\"figures/UCB.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TS (Thompson Sampling) Algorithm\n",
    "<img src=\"figures/TS.jpg\" width=\"50%\" align='left'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Now suppose we obtain the parameters of the Bernoulli distributions from an oracle, which are shown in the following table. Choose $N=5000$ and compute the theoretically maximized expectation of aggregate rewards over $N$ time slots. We call it the oracle value. Note that these parameters $\\theta_{j}, j \\in \\{1,2,3\\}$ and oracle values are unknown to all bandit algorithms.\n",
    "\n",
    "| Arm $j$ | 1   | 2   | 3   |\n",
    "|---------|-----|-----|-----|\n",
    "| $\\theta_j$ | 0.7 | 0.5 | 0.4 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the value of $\\theta_j$ is given, then we just have to choose the Arm 1 for we have a probability of getting reward to $0.7$, which is the maximum value among the three arms. Thus, the expectation of aggregate rewards over $5000$ slots would be \n",
    "$$N\\cdot \\theta_1 = 0.7 \\cdot 5000 = 3500 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement aforemented three classical bandit algorithms with following settings: \n",
    "   \n",
    "\t- $N=5000$\n",
    "\t- $\\epsilon$-greedy with $\\epsilon \\in \\{0.1, 0.5, 0.9\\}$.\n",
    "\t- UCB with $c \\in \\{1,5,10\\}$.\n",
    "\t- TS with\n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(1,1),(\\alpha_2,\\beta_2)=(1,1),(\\alpha_3,\\beta_3)=(1,1)\\right\\}$ \n",
    "    \t- $\\left\\{(\\alpha_1,\\beta_1)=(601,401),(\\alpha_2,\\beta_2)=(401,601),(\\alpha_3,\\beta_3)=(2,3)\\right\\}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random, math, copy\n",
    "### Import more packages if you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of epsilon-Greedy:\n",
    "### n is the number of time slots, epsilon is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def greedy(n, epsilon) -> tuple[int, np.ndarray]:\n",
    "    theta = np.array([0.7, 0.5, 0.4], dtype=float)   # used to generate rewards\n",
    "    theta_guess = np.array([0, 0, 0], dtype=float)  #initilize the vector we will adjust\n",
    "    count = np.array([0, 0, 0], dtype=int)  #init\n",
    "    total_r = 0   # this the sum of all rewards\n",
    "    for t in range(0, n):\n",
    "        choice = np.random.uniform(0, 1)    # used to choose I(t)\n",
    "        # arm is I(t)\n",
    "        arm = np.argmax(theta_guess) if choice > epsilon else np.random.choice([0, 1, 2])\n",
    "        count[arm] += 1\n",
    "        r = np.random.binomial(1, theta[arm])   # generate the reward\n",
    "        theta_guess[arm] += 1 / count[arm] * (r - theta_guess[arm])\n",
    "        total_r += r\n",
    "    return total_r, theta_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of UCB Algorithm:\n",
    "### n is the number of time slots, c is the parameter of the algorithm\n",
    "### return the total reward\n",
    "def UCB(n, c) -> tuple[int, np.ndarray]:\n",
    "    theta = np.array([0.7, 0.5, 0.4], dtype=float)  # used to generate rewards\n",
    "    theta_guess = np.array([0, 0, 0], dtype=float)\n",
    "    for i in range(3):\n",
    "        theta_guess[i] = np.random.binomial(1, theta[i])    # initilize the thetas\n",
    "    count = np.array([1, 1, 1], dtype=int)\n",
    "    total_r = 0 \n",
    "    for t in range(4, n+1):\n",
    "        # arm is the I(t)\n",
    "        arm = np.argmax(theta_guess + c * np.sqrt(2 * np.log(t) / count))\n",
    "        count[arm] += 1\n",
    "        r = np.random.binomial(1, theta[arm])   # reward\n",
    "        theta_guess[arm] += 1 / count[arm] * (r - theta_guess[arm])\n",
    "        total_r += r\n",
    "    return total_r, theta_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implementation of TS Algorithm\n",
    "### n is the number of time slots, a and b are the parameters of the algorithm\n",
    "### return the total reward\n",
    "\n",
    "# a = [a1, a2, a3] and b = [b1, b2, b3]\n",
    "def TS(n, a, b) -> tuple[int, np.ndarray]:\n",
    "    theta = np.array([0.7, 0.5, 0.4], dtype=float)\n",
    "    beta = np.array([(a[0], b[0]), (a[1], b[1]), (a[2], b[2])], dtype=int)\n",
    "    total_r = 0\n",
    "    for t in range(0, n):\n",
    "        # use this vector to store the expectation of beta distributions\n",
    "        E_theta = np.array([0, 0, 0], dtype=float)  \n",
    "        for j in range(3):\n",
    "            E_theta[j] = beta[j][0]/(beta[j][0] + beta[j][1])\n",
    "        arm = np.argmax(E_theta)\n",
    "        r = np.random.binomial(1, theta[arm]) # reward\n",
    "        beta[arm][0] += r\n",
    "        beta[arm][1] += 1- r\n",
    "        total_r += r\n",
    "    return total_r, beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Regard each of the above setting in problem 2 of Part I as an experiment (in total $8$ experiments).\n",
    "Run each experiment $200$ independent trials (change the random seed).\n",
    "Plot the final result (in terms of rewards and regrets) averaged over these $200$ trials."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When epsilon = 0.1, the averaged reward is 3412.555000, thetas are [0.69963108 0.49448575 0.40367482]\n",
      "When epsilon = 0.5, the averaged reward is 3080.555000, thetas are [0.69996539 0.49876338 0.39977516]\n",
      "When epsilon = 0.9, the averaged reward is 2750.300000, thetas are [0.70079345 0.50073627 0.39887706]\n"
     ]
    }
   ],
   "source": [
    "## greedy\n",
    "epsilons = [0.1, 0.5, 0.9]\n",
    "total_rs = np.array([0, 0, 0], dtype=int)\n",
    "thetas = np.zeros([3, 3], dtype=float) \n",
    "for k in range(3):\n",
    "    for i in range(num_trials):\n",
    "        r, theta = greedy(5000, epsilons[k])\n",
    "        total_rs[k] += r\n",
    "        thetas[k] += theta\n",
    "greedy_rewards = total_rs / num_trials\n",
    "greedy_thetas = thetas / num_trials\n",
    "for i in range(3):\n",
    "    print(\"When epsilon = {}, the averaged reward is {:.6f}, thetas are {}\" .format(epsilons[i], greedy_rewards[i], greedy_thetas[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When c = 1, the averaged reward is 3405.170000, thetas are [0.69914628 0.49002058 0.38650281]\n",
      "When c = 5, the averaged reward is 2976.625000, thetas are [0.69965383 0.49831795 0.39716002]\n",
      "When c = 10, the averaged reward is 2826.720000, thetas are [0.69956058 0.50176087 0.39950346]\n"
     ]
    }
   ],
   "source": [
    "## UCB\n",
    "c = [1, 5, 10]\n",
    "total_rs = np.array([0, 0, 0], dtype=int)\n",
    "thetas = np.zeros([3, 3], dtype=float) \n",
    "for k in range(3):\n",
    "    for i in range(num_trials):\n",
    "        r, theta = UCB(5000, c[k])\n",
    "        total_rs[k] += r\n",
    "        thetas[k] += theta\n",
    "ucb_rewards = total_rs / num_trials\n",
    "ucb_thetas = thetas / num_trials\n",
    "for i in range(3):\n",
    "    print(\"When c = {}, the averaged reward is {:.6f}, thetas are {}\" .format(c[i], ucb_rewards[i], ucb_thetas[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When (a1,b1) = (1,1), (a2,b2) = (1,1), (a3,b3) = (1,1), the total reward is 3344.9350,\n",
      " the thetas are [0.699717   0.49715673 0.41179002]\n",
      "When (a1,b1) = (601,401), (a2,b2) = (401,601), (a3,b3) = (2,3), the total reward is 3498.3200,\n",
      " the thetas are [0.68299234 0.4001996  0.4       ]\n"
     ]
    }
   ],
   "source": [
    "## Thompson\n",
    "a = np.array([[1, 1, 1], [601, 401, 2]])\n",
    "b = np.array([[1, 1, 1], [401, 601, 3]])\n",
    "total_rs = np.array([0, 0], dtype=int)\n",
    "betas = np.zeros([2, 3, 2], dtype=float)\n",
    "for k in range(2):\n",
    "    for i in range(num_trials):\n",
    "        r, beta = TS(5000, a[k], b[k])\n",
    "        total_rs[k] += r\n",
    "        betas[k] += beta\n",
    "ts_rewards = total_rs / num_trials\n",
    "ts_betas = betas / num_trials\n",
    "ts_thetas = np.zeros([2, 3], dtype=float)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        ts_thetas[i][j] = ts_betas[i][j][0] / (ts_betas[i][j][0] + ts_betas[i][j][1])\n",
    "print(\"When (a1,b1) = (1,1), (a2,b2) = (1,1), (a3,b3) = (1,1), the total reward is {:.4f},\\n the thetas are {}\" \n",
    "      .format(ts_rewards[0], ts_thetas[0]))\n",
    "print(\"When (a1,b1) = (601,401), (a2,b2) = (401,601), (a3,b3) = (2,3), the total reward is {:.4f},\\n the thetas are {}\" \n",
    "      .format(ts_rewards[1], ts_thetas[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute the gaps between the algorithm outputs (aggregated rewards over $N$ time slots) and the oracle value. Compare the numerical results of $\\epsilon$-greedy, UCB, and TS.\n",
    "   - Which one is the best?\n",
    "   - Discuss the impacts of $\\epsilon$, $c$, and $\\alpha_{j}$, $\\beta_{j}$, respectively. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.4. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Give your understanding of the exploration-exploitation trade-off in bandit algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part I**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. We implicitly assume the reward distribution of these three arms are independent. How about the dependent case?\n",
    "\tCan you design an algorithm to exploit such information to obtain a better result?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 6 in Part I**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 1.6. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Bayesian Bandit Algorithms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two arms which may be pulled repeatedly in any order.\n",
    "Each pull may result in either a success or a failure.\n",
    "The sequence of successes and failures which results from pulling arm $i$ ($i \\in \\{1, 2\\}$) forms a Bernoulli process with unknown success probability $\\theta_{i}$.\n",
    "A success at the $t^{th}$ pull yields a reward $\\gamma^{t-1}$ ($0 < \\gamma <1$), while an unsuccessful pull yields a zero reward.\n",
    "At time zero, each $\\theta_{i}$ has a Beta prior distribution with two parameters $\\alpha_{i}, \\beta_{i}$ and these distributions are independent for different arms.\n",
    "These prior distributions are updated to posterior distributions as arms are pulled.\n",
    "Since the class of Beta distributions is closed under Bernoulli sampling, posterior distributions are all Beta distributions.\n",
    "How should the arm to pull next in each time slot be chosen to maximize the total expected reward from an infinite sequence of pulls?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. \tOne intuitive policy suggests that in each time slot we should pull the arm for which the current expected value of $\\theta_{i}$ is the largest.\n",
    "\tThis policy behaves very good in most cases.\n",
    "\tPlease design simulations to check the behavior of this policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 1 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.1. Feel free to insert more blocks or helper functions if you need."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. However, such intuitive policy is unfortunately not optimal.\n",
    "\tPlease provide an example to show why such policy is not optimal. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 2 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. For the expected total reward under an optimal policy, show that the following recurrence equation holds:\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\t\\begin{aligned}\n",
    "\t\t\tR_{1}(\\alpha_{1},\\beta_{1}) \n",
    "\t\t\t= & \\frac{\\alpha_{1}}{\\alpha_{1}+\\beta_{1}} [1+\\gamma R(\\alpha_{1} + 1, \\beta_{1}, \\alpha_{2}, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{1}}{\\alpha_{1} + \\beta_{1}} [\\gamma R(\\alpha_{1}, \\beta_{1} + 1, \\alpha_{2}, \\beta_{2})]; \\\\\n",
    "\t\t\tR_{2}(\\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\frac{\\alpha_{2}}{\\alpha_{2} + \\beta_{2}} [1 + \\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2} + 1, \\beta_{2})] \\\\\n",
    "\t\t\t\t& + \\frac{\\beta_{2}}{\\alpha_{2} + \\beta_{2}} [\\gamma R(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2} + 1)]; \\\\\n",
    "\t\t\tR(\\alpha_{1}, \\beta_{1}, \\alpha_{2}, \\beta_{2}) \n",
    "\t\t\t= & \\max \\left\\{ R_{1}(\\alpha_{1}, \\beta_{1}), R_{2}(\\alpha_{2}, \\beta_{2}) \\right\\}.\n",
    "\t\t\\end{aligned}  \t\n",
    "\t\\end{equation*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 3 in Part II**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For the above equations, how to solve it exactly or approximately? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 4 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.4 if needed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the optimal policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your anwser of problem 5 in Part II**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code for problem 2.5. Feel free to insert more blocks or helper functions if you need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
